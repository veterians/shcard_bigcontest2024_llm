import os
import numpy as np
import pandas as pd

from transformers import AutoTokenizer, AutoModel
import torch
from tqdm import tqdm
import faiss

import streamlit as st

# ê²½ë¡œ ì„¤ì •
data_path = './data'
module_path = './modules'


# Gemini ì„¤ì •
import google.generativeai as genai

# import shutil
# os.makedirs("/root/.streamlit", exist_ok=True)
# shutil.copy("secrets.toml", "/root/.streamlit/secrets.toml")
# ì–´ë µë‹¤ ì–´ë ¤ì›Œ...z..
# GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]

genai.configure(api_key="AIzaSyDIYgGSW1JnXWtqmPjSA7YR9-A4xz7-YYg")

# Gemini ëª¨ë¸ ì„ íƒ
model = genai.GenerativeModel("gemini-1.5-flash")


# CSV íŒŒì¼ ë¡œë“œ
## ìì²´ ì „ì²˜ë¦¬ë¥¼ ê±°ì¹œ ë°ì´í„° íŒŒì¼ í™œìš©
csv_file_path = "JEJU_MCT_DATA_modified.csv"
df = pd.read_csv(os.path.join(data_path, csv_file_path))

# ìµœì‹ ì—°ì›” ë°ì´í„°ë§Œ ê°€ì ¸ì˜´
df = df[df['ê¸°ì¤€ì—°ì›”'] == df['ê¸°ì¤€ì—°ì›”'].max()].reset_index(drop=True)


# Streamlit App UI

st.set_page_config(page_title="ğŸŠì°¸ì‹ í•œ ì œì£¼ ë§›ì§‘!")

# Replicate Credentials
with st.sidebar:
    st.title("ğŸŠì°¸ì‹ í•œ! ì œì£¼ ë§›ì§‘")

    st.write("")
     
    st.subheader("ì–¸ë“œë ˆ ê°€ì‹ ë””ê°€?")

    # selectbox ë ˆì´ë¸” ê³µë°± ì œê±°
    st.markdown(
        """
        <style>
        .stSelectbox label {  /* This targets the label element for selectbox */
            display: none;  /* Hides the label element */
        }
        .stSelectbox div[role='combobox'] {
            margin-top: -20px; /* Adjusts the margin if needed */
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    time = st.sidebar.selectbox("", ["ì•„ì¹¨", "ì ì‹¬", "ì˜¤í›„", "ì €ë…", "ë°¤"], key="time")

    st.write("")

    st.subheader("ì–´ë“œë ˆê°€ ë§˜ì— ë“œì‹ ë””ê°€?")

    # radio ë ˆì´ë¸” ê³µë°± ì œê±°
    st.markdown(
        """
        <style>
        .stSelectbox label {  /* This targets the label element for selectbox */
            display: none;  /* Hides the label element */
        }
        .stSelectbox div[role='combobox'] {
            margin-top: -20px; /* Adjusts the margin if needed */
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    place = st.sidebar.selectbox("", ["ì œì£¼", "ì„œê·€í¬", "ì„±ì‚°"], key="place")

    st.write("")


    st.write("")

     # radio ë ˆì´ë¸” ê³µë°± ì œê±°
    st.markdown(
        """
        <style>
        .stRadio > label {
            display: none;
        }
        .stRadio > div {
            margin-top: -20px;
        }
        </style>
        """,
        unsafe_allow_html=True
    )

st.title("í˜¼ì € ì˜µì„œì˜ˆ!ğŸ‘‹")
st.subheader("êµ°ë§›ë‚œ ì œì£¼ ë°¥ì§‘ğŸ§‘â€ğŸ³ ì¶”ì²œí•´ë“œë¦´ê²Œì˜ˆ")

st.write("")

st.write("#í‘ë¼ì§€ #ê°ˆì¹˜ì¡°ë¦¼ #ì˜¥ë”êµ¬ì´ #ê³ ì‚¬ë¦¬í•´ì¥êµ­ #ì „ë³µëšë°°ê¸° #í•œì¹˜ë¬¼íšŒ #ë¹™ë–¡ #ì˜¤ë©”ê¸°ë–¡..ğŸ¤¤")

st.write("")

image_path = "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRTHBMuNn2EZw3PzOHnLjDg_psyp-egZXcclWbiASta57PBiKwzpW5itBNms9VFU8UwEMQ&usqp=CAU"
image_html = f"""
<div style="display: flex; justify-content: center;">
    <img src="{image_path}" alt="centered image" width="50%">
</div>
"""
st.markdown(image_html, unsafe_allow_html=True)

st.write("")

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}]
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)


# RAG

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# Hugging Faceì˜ ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "jhgan/ko-sroberta-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_model = AutoModel.from_pretrained(model_name).to(device)

print(f'Device is {device}.')


# FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜
def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index.index')):
    """
    FAISS ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.

    Parameters:
    index_path (str): ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ.

    Returns:
    faiss.Index: ë¡œë“œëœ FAISS ì¸ë±ìŠ¤ ê°ì²´.
    """
    if os.path.exists(index_path):
        # ì¸ë±ìŠ¤ íŒŒì¼ì—ì„œ ë¡œë“œ
        index = faiss.read_index(index_path)
        print(f"FAISS ì¸ë±ìŠ¤ê°€ {index_path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return index
    else:
        raise FileNotFoundError(f"{index_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# í…ìŠ¤íŠ¸ ì„ë² ë”©
def embed_text(text):
    # í† í¬ë‚˜ì´ì €ì˜ ì¶œë ¥ë„ GPUë¡œ ì´ë™
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)
    with torch.no_grad():
        # ëª¨ë¸ì˜ ì¶œë ¥ì„ GPUì—ì„œ ì—°ì‚°í•˜ê³ , í•„ìš”í•œ ë¶€ë¶„ì„ ê°€ì ¸ì˜´
        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings.squeeze().cpu().numpy()  # ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ê³  numpy ë°°ì—´ë¡œ ë³€í™˜

# ì„ë² ë”© ë¡œë“œ
embeddings = np.load(os.path.join(module_path, 'embeddings_array_file.npy'))

### Generator ì˜ì—­ ###

def generate_response_with_faiss(question, df, embeddings, model, embed_text, time, local_choice, index_path=os.path.join(module_path, 'faiss_index.index'), max_count=10, k=3, print_prompt=True):
    # ë°ì´í„° í”„ë ˆì„ ì´ˆê¸°í™”
    filtered_df = df
    
    # FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ê³  ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    index = load_faiss_index(index_path)
    query_embedding = embed_text(question).reshape(1, -1)
    
    # ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰
    distances, indices = index.search(query_embedding, k * 3)
    filtered_df = filtered_df.iloc[indices[0, :]].copy().reset_index(drop=True)

    # ì‹œê°„ëŒ€ í•„í„°ë§ ì¡°ê±´ ì¶”ê°€
    if time == 'ì•„ì¹¨':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(5, 12)))].reset_index(drop=True)
    elif time == 'ì ì‹¬':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(12, 14)))].reset_index(drop=True)
    elif time == 'ì˜¤í›„':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(14, 18)))].reset_index(drop=True)
    elif time == 'ì €ë…':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(18, 23)))].reset_index(drop=True)
    elif time == 'ë°¤':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in [23, 24, 1, 2, 3, 4]))].reset_index(drop=True)

    # í•„í„°ë§ í›„ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì•Œë¦¼ ë©”ì‹œì§€ ë°˜í™˜
    if filtered_df.empty:
        return f"í˜„ì¬ ì„ íƒí•˜ì‹  ì‹œê°„ëŒ€({time})ì—ëŠ” ì˜ì—…í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    # ìƒìœ„ kê°œì˜ ê²°ê³¼ë§Œ ì‚¬ìš©
    filtered_df = filtered_df.reset_index(drop=True).head(k)

    # í˜„ì§€ì¸ ë§›ì§‘ì¸ì§€ ê´€ê´‘ê° ë§›ì§‘ì¸ì§€ í•„í„°ë§ ì¶”ê°€
    if local_choice == 'ì œì£¼ë„ë¯¼ ë§›ì§‘':
        local_choice = 'ì œì£¼ë„ë¯¼(í˜„ì§€ì¸) ë§›ì§‘'
    elif local_choice == 'ê´€ê´‘ê° ë§›ì§‘':
        local_choice = 'í˜„ì§€ì¸ ë¹„ì¤‘ì´ ë‚®ì€ ê´€ê´‘ê° ë§›ì§‘'

    # ì‚¬ìš©ì ë§ì¶¤í˜• ì¶”ì²œ ì •ë³´ ìƒì„±
    reference_info = ""
    for idx, row in filtered_df.iterrows():
        # ê¸°ë³¸ ê°€ê²Œ ì •ë³´ êµ¬ì„±
        store_name = row['ê°€ë§¹ì ëª…']
        location = row['ê°€ë§¹ì ì£¼ì†Œ']
        category = row['ê°€ë§¹ì ì—…ì¢…']
        opening_date = row['ê°€ë§¹ì ê°œì„¤ì¼ì']
        usage_amount_range = row['ì´ìš©ê¸ˆì•¡êµ¬ê°„']
        local_ratio = row['í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘']

        # ì¶”ì²œ ì •ë³´ í¬ë§·
        reference_info += f"**{store_name}**\nìœ„ì¹˜: {location}\nì—…ì¢…: {category}\nê°œì—…ì¼: {opening_date}\n"
        reference_info += f"ê°€ê²©ëŒ€: {usage_amount_range}\n"

        # í˜„ì§€ì¸ ë¹„ìœ¨ì„ í†µí•´ í˜„ì§€ì¸ ë§›ì§‘ì¸ì§€ ê´€ê´‘ê° ë§›ì§‘ì¸ì§€ ì„¤ëª… ì¶”ê°€
        if local_ratio > 0.5:
            reference_info += "ì´ê³³ì€ í˜„ì§€ì¸ë“¤ì´ ìì£¼ ì°¾ëŠ” ë§›ì§‘ì…ë‹ˆë‹¤. í˜„ì§€ì¸ ë¹„ìœ¨ì´ ë†’ì•„ ì§„ì •í•œ ì œì£¼ ë§›ì„ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ê³³ìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤.\n"
        else:
            reference_info += "ì´ê³³ì€ ê´€ê´‘ê°ë“¤ì—ê²Œ ì¸ê¸°ê°€ ë§ìŠµë‹ˆë‹¤. ì œì£¼ì—ì„œ í¸ì•ˆíˆ ê´€ê´‘ì„ ì¦ê¸°ë©° ìŒì‹ì„ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ê³³ì…ë‹ˆë‹¤.\n"

        # ë©”ë‰´ ë° ê¸°íƒ€ ì •ë³´
        reference_info += f"ì£¼ìš” ë©”ë‰´ ë° íŠ¹ì§•: {row['text']}\n\n"

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = f"ì§ˆë¬¸: {question} íŠ¹íˆ {local_choice}ì„ ì„ í˜¸í•´\nì°¸ê³ í•  ì •ë³´:\n{reference_info}\nì‘ë‹µ:"

    # í”„ë¡¬í”„íŠ¸ê°€ ì˜ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if print_prompt:
        print('-----------------------------'*3)
        print(prompt)
        print('-----------------------------'*3)

    # ëª¨ë¸ì„ í†µí•´ ì‘ë‹µ ìƒì„±
    response = model.generate_content(prompt)
    return response

# User-provided prompt
if prompt := st.chat_input(): # (disabled=not replicate_api):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            # response = generate_llama2_response(prompt)
            response = generate_response_with_faiss(prompt, df, embeddings, model, embed_text, time, local_choice)
            placeholder = st.empty()
            full_response = ''

            # ë§Œì•½ responseê°€ GenerateContentResponse ê°ì²´ë¼ë©´, ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
            if isinstance(response, str):
                full_response = response
            else:
                full_response = response.text  # response ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ë¶€ë¶„ ì¶”ì¶œ

            # for item in response:
            #     full_response += item
            #     placeholder.markdown(full_response)

            placeholder.markdown(full_response)
    message = {"role": "assistant", "content": full_response}
    st.session_state.messages.append(message)